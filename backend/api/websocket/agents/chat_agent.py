"""Chat agent for Requirements Engineering Assistant."""

import asyncio
import json
import traceback
from datetime import datetime
from typing import Optional, List

from api.websocket.agents.base_agent import BaseAgent
from api.core.models import Message
from api.services.conversation import ConversationService
from api.core.db import async_session

# Import Google Gemini API
try:
    import google.generativeai as genai
    from api.core.config import settings
    
    GENAI_API_KEY = settings.GENAI_API_KEY
    MODEL = settings.LLM_MODEL
    
    if GENAI_API_KEY:
        genai.configure(api_key=GENAI_API_KEY)
except Exception as e:
    genai = None
    GENAI_API_KEY = None
    MODEL = None
    print(f"Warning: Could not load Gemini API: {e}")


class ChatAgent(BaseAgent):
    """Business Analysis Assistant - Chuy√™n gia ph√¢n t√≠ch nghi·ªáp v·ª• v√† y√™u c·∫ßu kinh doanh.
    
    üéØ SCOPE: CH·ªà h·ªó tr·ª£ Business Analysis & Use Case Analysis
    
    ‚úÖ Core Capabilities:
    - Ph√¢n t√≠ch y√™u c·∫ßu nghi·ªáp v·ª• (Business Requirements Analysis)
    - X√°c ƒë·ªãnh y√™u c·∫ßu kinh doanh (Business Requirements Specification)
    - Ph√¢n t√≠ch Use Cases (Use Case Analysis & Modeling)
    - T·∫°o Context Diagram (System boundary, external actors)
    - T·∫°o Use Case Diagram (Actors, use cases, relationships)
    - Vi·∫øt Use Case Specifications (Main flow, alternative flows, preconditions, postconditions)
    - Ph√¢n t√≠ch Stakeholders (Identify, classify, analyze needs)
    - Ph√¢n t√≠ch Business Process (As-Is, To-Be process mapping)
    - Requirements Prioritization (MoSCoW, Business Value)
    - Gap Analysis (Current vs Desired state)
    
    ‚ùå KH√îNG h·ªó tr·ª£:
    - Coding/Programming/Development
    - Technical architecture design
    - Database schema design
    - API/Backend implementation
    - Frontend UI/UX design details
    - Infrastructure/DevOps setup
    - Testing automation
    - Project management tasks
    - General chatbot/casual conversation
    
    üìã Main Workflow:
    1. Thu th·∫≠p business requirements v√† use cases t·ª´ user
    2. Ph√¢n t√≠ch nghi·ªáp v·ª•, x√°c ƒë·ªãnh actors v√† use cases
    3. T√¨m conflicts, gaps, ambiguity trong requirements
    4. ∆Øu ti√™n requirements theo business value
    5. T·∫°o Context Diagram + Use Case Diagram
    6. Generate Use Case Specifications
    7. L∆∞u analysis v√†o DB v·ªõi embeddings ƒë·ªÉ recall
    """

    def __init__(self, session_id: str, user_id: Optional[int] = None, agent_id: Optional[int] = None):
        super().__init__(session_id)
        self.user_id = user_id or 1
        self.agent_id = agent_id or 1
        self.conversation_id: Optional[int] = None
        self.conversation_service = ConversationService()
        
        # State - NO memory cache for messages, all from DB
        self.is_first_message = True  # Track first user message for auto-naming
        
        # Temporary cache for current tool execution (within single request)
        self.last_pipeline_result = {
            "stories": [],
            "analysis": {},
            "requirements": [],
            "validation_issues": [],
            "diagram": "",
            "report": {}
        }

    async def initialize_conversation(self, conversation_name: Optional[str] = None):
        """Initialize conversation in database and load existing context."""
        async with async_session() as db:
            if not self.conversation_id:
                # Create conversation with temporary name (will be updated after first message)
                conversation = await self.conversation_service.create_conversation(
                    db=db,
                    name=conversation_name or f"New Chat {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                    user_id=self.user_id,
                    is_shared=False
                )
                self.conversation_id = conversation.id
                
                await self.conversation_service.create_conversation_agent(
                    db=db,
                    conversation_id=self.conversation_id,
                    agent_id=self.agent_id,
                    is_active=True
                )
            else:
                # Load existing conversation context
                self.is_first_message = False  # Existing conversation, not first message
                await self._load_conversation_context(db)

    async def handle_message(self, message: str) -> str:
        """Handle incoming message - Load context from DB, process, save back to DB."""
        if not self.conversation_id:
            await self.initialize_conversation()
        
        # 1. Save user message
        await self._save_message(role=1, content=message, user_id=self.user_id)
        
        # 2. Auto-generate conversation name from first message
        if self.is_first_message:
            await self._auto_name_conversation(message)
            self.is_first_message = False
        
        # 3. Load conversation summary and recent messages from DB
        conversation_summary = await self._load_conversation_summary()
        recent_messages = await self._load_recent_messages(limit=10)
        
        # 4. Generate response with loaded context (summary + recent messages)
        response = await self._generate_response(message, conversation_summary, recent_messages)
        
        # 5. Save response
        await self._save_message(role=2, content=response, agent_id=self.agent_id)
        
        # 6. Update conversation summary periodically (every 5 messages)
        await self._update_conversation_summary_if_needed()
        
        return response

    async def _save_message(
        self, 
        role: int, 
        content: str, 
        user_id: Optional[int] = None, 
        agent_id: Optional[int] = None
    ):
        """Save message to database."""
        async with async_session() as db:
            message = Message(
                role=role,
                content=content,
                content_type=1,
                message_type=1,
                conversation_id=self.conversation_id,
                user_id=user_id,
                agent_id=agent_id,
                created_at=datetime.utcnow(),
                status=1
            )
            db.add(message)
            await db.commit()
    
    async def _auto_name_conversation(self, first_message: str):
        """Auto-generate conversation name from first user message using Gemini."""
        try:
            if not genai or not GENAI_API_KEY:
                return
            
            # Use Gemini to generate a short, descriptive name (max 50 chars)
            prompt = f"""T·∫°o t√™n ng·∫Øn g·ªçn (t·ªëi ƒëa 50 k√Ω t·ª±) cho cu·ªôc h·ªôi tho·∫°i d·ª±a tr√™n tin nh·∫Øn ƒë·∫ßu ti√™n c·ªßa ng∆∞·ªùi d√πng.
Ch·ªâ tr·∫£ v·ªÅ t√™n, kh√¥ng gi·∫£i th√≠ch.

Tin nh·∫Øn: {first_message}

V√≠ d·ª•:
- "Ph√¢n t√≠ch use case ƒëƒÉng nh·∫≠p" ‚Üí "Use Case ƒêƒÉng Nh·∫≠p"
- "Requirements cho h·ªá th·ªëng qu·∫£n l√Ω th∆∞ vi·ªán" ‚Üí "H·ªá Th·ªëng Th∆∞ Vi·ªán"
- "T√¥i mu·ªën l√†m app b√°n h√†ng" ‚Üí "App B√°n H√†ng"

T√™n cu·ªôc h·ªôi tho·∫°i:"""

            model = genai.GenerativeModel(MODEL)
            response = await asyncio.to_thread(
                model.generate_content,
                prompt,
                generation_config={
                    "temperature": 0.3,
                    "max_output_tokens": 50,
                }
            )
            
            if response and response.text:
                conversation_name = response.text.strip()
                # Remove quotes if present
                conversation_name = conversation_name.strip('"').strip("'")
                # Truncate to 50 chars if needed
                if len(conversation_name) > 50:
                    conversation_name = conversation_name[:47] + "..."
                
                # Update conversation name in DB
                async with async_session() as db:
                    await self.conversation_service.update_conversation(
                        db=db,
                        conversation_id=self.conversation_id,
                        name=conversation_name
                    )
        except Exception as e:
            print(f"Error auto-naming conversation: {e}")
            # Silently fail, keep default name
    
    async def _load_conversation_context(self, db):
        """Load existing conversation summary from DB (called on reconnect)."""
        conversation = await self.conversation_service.get_conversation(db, self.conversation_id)
        if conversation and conversation.summary:
            # Summary exists, will be used in context when generating responses
            return conversation.summary
        return None
    
    async def _load_conversation_summary(self) -> Optional[str]:
        """Load conversation summary for context."""
        async with async_session() as db:
            conversation = await self.conversation_service.get_conversation(db, self.conversation_id)
            if conversation and conversation.summary:
                return conversation.summary
            return None
    
    async def _load_recent_messages(self, limit: int = 10) -> List[dict]:
        """Load recent messages from DB for context."""
        async with async_session() as db:
            from sqlalchemy import select
            
            stmt = select(Message).where(
                Message.conversation_id == self.conversation_id,
                Message.status == 1
            ).order_by(Message.created_at.desc()).limit(limit)
            
            result = await db.execute(stmt)
            messages = result.scalars().all()
            
            # Reverse to get chronological order
            messages = list(reversed(messages))
            
            # Format for context
            context = []
            for msg in messages:
                role_name = "user" if msg.role == 1 else "assistant" if msg.role == 2 else "system"
                context.append({
                    "role": role_name,
                    "content": msg.content,
                    "timestamp": msg.created_at.isoformat() if msg.created_at else None
                })
            
            return context
    
    async def _update_conversation_summary_if_needed(self):
        """Update conversation summary periodically (every 5 messages)."""
        async with async_session() as db:
            from sqlalchemy import select, func
            
            # Count messages in conversation
            stmt = select(func.count(Message.id)).where(
                Message.conversation_id == self.conversation_id,
                Message.status == 1
            )
            result = await db.execute(stmt)
            count = result.scalar()
            
            # Update summary every 5 messages
            if count % 5 == 0:
                # Load all messages
                all_messages = await self._load_recent_messages(limit=100)
                
                # Generate summary text
                summary_text = f"""# Conversation Summary
Updated: {datetime.now().strftime('%Y-%m-%d %H:%M')}
Total Messages: {count}

## Recent Exchanges:
"""
                for msg in all_messages[-10:]:  # Last 10 messages
                    role = msg['role'].upper()
                    content = msg['content'][:200]  # Truncate long messages
                    summary_text += f"\n**{role}**: {content}...\n"
                
                # Generate embedding
                embedding = await self._generate_embedding(summary_text)
                
                # Save to conversation
                await self._save_conversation_summary(summary_text, embedding)
    
    async def _save_conversation_summary(self, summary: str, embedding: Optional[List[float]] = None):
        """Save conversation summary and embedding to DB."""
        async with async_session() as db:
            conversation = await self.conversation_service.get_conversation(db, self.conversation_id)
            if conversation:
                conversation.summary = summary
                if embedding:
                    conversation.summary_embedding = embedding
                conversation.last_updated = datetime.utcnow()
                db.add(conversation)
                await db.commit()
    
    async def _generate_embedding(self, text: str) -> List[float]:
        """Generate embedding using Gemini API."""
        try:
            if not genai or not GENAI_API_KEY:
                return []
            
            # Use Gemini embedding API
            result = await asyncio.to_thread(
                genai.embed_content,
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_document"
            )
            
            return result['embedding']
        except Exception as e:
            print(f"Error generating embedding: {e}")
            return []
    
    async def _search_similar_conversations(self, query: str, top_k: int = 5):
        """Search similar conversations using embeddings."""
        try:
            # Generate query embedding
            query_embedding = await self._generate_embedding(query)
            if not query_embedding:
                return []
            
            # Search in DB using cosine similarity
            # TODO: Implement vector similarity search in PostgreSQL with pgvector
            # For now, return empty
            return []
        except Exception as e:
            print(f"Error searching conversations: {e}")
            return []

    async def _generate_response(
        self, 
        message: str, 
        summary: Optional[str], 
        recent_messages: List[dict]
    ) -> str:
        """Generate response using Gemini with loaded context from DB."""
        text = message.strip()
        text_lower = text.lower()
        
        # Only handle ping command
        if text_lower == "ping":
            return "pong"
        
        # Use Gemini to understand intent and route to appropriate action
        if genai and GENAI_API_KEY:
            return await self._call_gemini_orchestrator(text, summary, recent_messages)
        
        return "Xin l·ªói, t√¥i ƒëang g·∫∑p v·∫•n ƒë·ªÅ k·∫øt n·ªëi v·ªõi AI. Vui l√≤ng th·ª≠ l·∫°i."

    def _is_requirement(self, text: str) -> bool:
        """Check if text is a requirement (legacy - not used with Gemini orchestrator)."""
        text_lower = text.lower()
        patterns = [
            "story:", "as a ", "as an ", "given ", "when ", "then ",
            "acceptance criteria:", "requirement:", "the system shall",
            "the system must", "the user can", "the user should"
        ]
        
        return any(p in text_lower for p in patterns)

    async def _run_pipeline(self) -> str:
        """Run requirements analysis pipeline using MCP servers (LEGACY - not used)."""
        try:
            # Load requirements from DB instead of memory
            recent_messages = await self._load_recent_messages(limit=50)
            raw_text = "\n\n".join([m['content'] for m in recent_messages if m['role'] == 'user'])
            reqs_count = len(recent_messages)
            
            # Import MCP adapter
            from api.services import mcp_adapter
            
            # Step 1: Collector - ingest raw text
            ing_resp = await asyncio.to_thread(
                mcp_adapter.call_mcp,
                "mcp_collector",
                "ingest_raw",
                {"items": [raw_text]}
            )
            
            if ing_resp.get("error"):
                return f"‚ùå L·ªói Collector (ingest): {ing_resp.get('error')}"
            
            chunks = ing_resp.get("response", {}).get("chunks") or ing_resp.get("chunks") or []
            
            # Step 2: Collector - normalize chunks
            norm_resp = await asyncio.to_thread(
                mcp_adapter.call_mcp,
                "mcp_collector",
                "normalize",
                {"chunks": chunks}
            )
            
            if norm_resp.get("error"):
                return f"‚ùå L·ªói Collector (normalize): {norm_resp.get('error')}"
            
            norm_chunks = norm_resp.get("response", {}).get("chunks") or norm_resp.get("chunks") or []
            
            # Step 3: Collector - extract stories
            ext_resp = await asyncio.to_thread(
                mcp_adapter.call_mcp,
                "mcp_collector",
                "extract_stories",
                {"chunks": norm_chunks}
            )
            
            if ext_resp.get("error"):
                return f"‚ùå L·ªói Collector (extract): {ext_resp.get('error')}"
            
            stories = ext_resp.get("response", {}).get("stories") or ext_resp.get("stories") or []
            
            # Step 4: Analyzer - analyze stories
            anl_resp = await asyncio.to_thread(
                mcp_adapter.call_mcp,
                "mcp_analyzer",
                "analyze_stories",
                {"stories": stories}
            )
            
            if anl_resp.get("error"):
                return f"‚ùå L·ªói Analyzer: {anl_resp.get('error')}"
            
            analysis = anl_resp.get("response", {}) or anl_resp
            
            # Step 5: Requirement - identify requirements
            idr_resp = await asyncio.to_thread(
                mcp_adapter.call_mcp,
                "mcp_requirement",
                "identify_requirements",
                {"stories": stories, "analysis": analysis}
            )
            
            if idr_resp.get("error"):
                return f"‚ùå L·ªói Requirement (identify): {idr_resp.get('error')}"
            
            requirements = idr_resp.get("response", {}).get("requirements") or idr_resp.get("requirements") or []
            
            # Step 6: Requirement - prioritize
            pri_resp = await asyncio.to_thread(
                mcp_adapter.call_mcp,
                "mcp_requirement",
                "prioritize",
                {"requirements": requirements}
            )
            
            if pri_resp.get("error"):
                return f"‚ùå L·ªói Requirement (prioritize): {pri_resp.get('error')}"
            
            prioritized = pri_resp.get("response", {}) or pri_resp
            
            # Step 7: Reporter - build final report with context diagram
            rep_resp = await asyncio.to_thread(
                mcp_adapter.call_mcp,
                "mcp_reporter",
                "build_final_report",
                {
                    "core_requirements": requirements,
                    "analyzer_output": analysis,
                    "project_id": f"project_{self.conversation_id}"
                }
            )
            
            if rep_resp.get("error"):
                return f"‚ùå L·ªói Reporter: {rep_resp.get('error')}"
            
            report = rep_resp.get("response") or rep_resp
            
            # Extract results
            stories_count = len(stories)
            reqs_count = len(requirements)
            
            # Get mermaid diagram from report
            mermaid_diagram = report.get("final_report_mermaid", "")
            markdown_report = report.get("final_report_markdown", "")
            
            # Cache pipeline results for future reference
            self.last_pipeline_result = {
                "stories": stories,
                "analysis": analysis,
                "requirements": requirements,
                "validation_issues": [],
                "diagram": mermaid_diagram,
                "report": report
            }
            
            # Create conversation summary for DB storage
            summary_text = f"""# Requirements Analysis Summary

## Project: project_{self.conversation_id}
## Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}

### Input Requirements ({reqs_count})
{raw_text[:500]}...

### Extracted Stories ({stories_count})
{json.dumps(stories[:3] if len(stories) > 3 else stories, indent=2, ensure_ascii=False)}

### Core Requirements ({len(requirements)})
{json.dumps(requirements, indent=2, ensure_ascii=False)}

### Analysis Results
{json.dumps(analysis, indent=2, ensure_ascii=False)}

### Context Diagram
```mermaid
{mermaid_diagram}
```

### Executive Summary
{markdown_report}
"""
            
            # Generate embedding from summary
            embedding = await self._generate_embedding(summary_text)
            
            # Save summary and embedding to conversation table
            await self._save_conversation_summary(summary_text, embedding)
            
            # Format result
            result = f"""‚úÖ Pipeline ph√¢n t√≠ch ho√†n t·∫•t!

üìä K·∫øt qu·∫£:
‚Ä¢ {reqs_count} requirements ban ƒë·∫ßu
‚Ä¢ {stories_count} stories ƒë∆∞·ª£c tr√≠ch xu·∫•t
‚Ä¢ {len(requirements)} core requirements ƒë∆∞·ª£c x√°c ƒë·ªãnh

üìà Context Diagram:
```mermaid
{mermaid_diagram}
```

ÔøΩ Executive Summary:
{markdown_report[:500]}...

ÔøΩüíæ ƒê√£ l∆∞u v√†o conversation #{self.conversation_id}
"""
            
            # Save full pipeline result to DB
            await self._save_message(
                role=3,
                content=json.dumps({
                    "type": "pipeline_result",
                    "project_id": f"project_{self.conversation_id}",
                    "collector": {
                        "chunks": len(chunks),
                        "normalized_chunks": len(norm_chunks),
                        "stories": stories
                    },
                    "analyzer": analysis,
                    "requirements": requirements,
                    "prioritized": prioritized,
                    "report": report
                }),
                agent_id=self.agent_id
            )
            
            return result
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"‚ùå L·ªói pipeline: {str(e)}\n\nChi ti·∫øt:\n{error_detail[:500]}"

    async def _call_gemini_orchestrator(
        self, 
        message: str, 
        summary: Optional[str],
        recent_messages: List[dict]
    ) -> str:
        """Call Gemini as orchestrator with function calling for MCP routing."""
        try:
            # Define function declarations matching MCP servers capabilities
            
            # Collector MCP functions
            ingest_raw_declaration = genai.protos.FunctionDeclaration(
                name="ingest_raw_requirements",
                description="Thu th·∫≠p v√† chu·∫©n h√≥a raw requirements t·ª´ user input. T·ª± ƒë·ªông g·ªçi khi user nh·∫≠p requirements.",
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "items": genai.protos.Schema(
                            type=genai.protos.Type.ARRAY,
                            items=genai.protos.Schema(type=genai.protos.Type.STRING),
                            description="Raw requirement text items"
                        )
                    },
                    required=["items"]
                )
            )
            
            # Analyzer MCP functions
            analyze_declaration = genai.protos.FunctionDeclaration(
                name="analyze_stories",
                description="Ph√¢n t√≠ch stories ƒë·ªÉ t√¨m issues, conflicts, suggestions. G·ªçi sau khi c√≥ stories t·ª´ collector.",
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "stories": genai.protos.Schema(
                            type=genai.protos.Type.ARRAY,
                            items=genai.protos.Schema(type=genai.protos.Type.OBJECT),
                            description="User stories c·∫ßn ph√¢n t√≠ch"
                        )
                    },
                    required=["stories"]
                )
            )
            
            # Requirement MCP functions
            identify_declaration = genai.protos.FunctionDeclaration(
                name="identify_requirements",
                description="X√°c ƒë·ªãnh v√† t·ªïng h·ª£p core requirements t·ª´ analyzed stories.",
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "stories": genai.protos.Schema(
                            type=genai.protos.Type.ARRAY,
                            items=genai.protos.Schema(type=genai.protos.Type.OBJECT),
                            description="Stories ƒë√£ ƒë∆∞·ª£c analyze"
                        )
                    },
                    required=["stories"]
                )
            )
            
            prioritize_declaration = genai.protos.FunctionDeclaration(
                name="prioritize_requirements",
                description="∆Øu ti√™n c√°c requirements theo ƒë·ªô quan tr·ªçng v√† urgency.",
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "requirements": genai.protos.Schema(
                            type=genai.protos.Type.ARRAY,
                            items=genai.protos.Schema(type=genai.protos.Type.OBJECT),
                            description="Requirements c·∫ßn prioritize"
                        )
                    },
                    required=["requirements"]
                )
            )
            
            # Reporter MCP function
            generate_report_declaration = genai.protos.FunctionDeclaration(
                name="generate_context_diagram",
                description="T·∫°o context diagram (Mermaid) t·ª´ prioritized requirements. G·ªçi cu·ªëi c√πng ƒë·ªÉ t·∫°o visualization.",
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "requirements": genai.protos.Schema(
                            type=genai.protos.Type.ARRAY,
                            items=genai.protos.Schema(type=genai.protos.Type.OBJECT),
                            description="Prioritized requirements"
                        )
                    },
                    required=["requirements"]
                )
            )
            
            # Validator MCP functions
            validate_req_declaration = genai.protos.FunctionDeclaration(
                name="validate_requirements",
                description="Validate requirements structure v√† completeness. G·ªçi sau khi prioritize ƒë·ªÉ ƒë·∫£m b·∫£o quality.",
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "requirements": genai.protos.Schema(
                            type=genai.protos.Type.ARRAY,
                            items=genai.protos.Schema(type=genai.protos.Type.OBJECT),
                            description="Requirements c·∫ßn validate"
                        )
                    },
                    required=["requirements"]
                )
            )
            
            # Vector MCP functions - for conversation context storage
            store_context_declaration = genai.protos.FunctionDeclaration(
                name="store_conversation_context",
                description="L∆∞u conversation context v√†o vector store ƒë·ªÉ c√≥ th·ªÉ retrieve sau n√†y. T·ª± ƒë·ªông g·ªçi sau khi ho√†n th√†nh pipeline.",
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "summary": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="Summary c·ªßa conversation"
                        ),
                        "requirements": genai.protos.Schema(
                            type=genai.protos.Type.ARRAY,
                            items=genai.protos.Schema(type=genai.protos.Type.OBJECT),
                            description="Requirements ƒë√£ x·ª≠ l√Ω"
                        ),
                        "diagram": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="Context diagram ƒë√£ t·∫°o"
                        )
                    },
                    required=["summary"]
                )
            )
            
            search_context_declaration = genai.protos.FunctionDeclaration(
                name="search_previous_context",
                description="T√¨m ki·∫øm previous conversation context t·ª´ vector store khi user h·ªèi v·ªÅ requirements tr∆∞·ªõc ƒë√≥.",
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "query": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="Search query"
                        ),
                        "top_k": genai.protos.Schema(
                            type=genai.protos.Type.INTEGER,
                            description="Number of results"
                        )
                    },
                    required=["query"]
                )
            )
            
            # Utility functions
            help_declaration = genai.protos.FunctionDeclaration(
                name="show_help",
                description="Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng chi ti·∫øt",
                parameters=genai.protos.Schema(type=genai.protos.Type.OBJECT, properties={})
            )
            
            clear_declaration = genai.protos.FunctionDeclaration(
                name="clear_requirements",
                description="X√≥a t·∫•t c·∫£ requirements ƒë√£ l∆∞u",
                parameters=genai.protos.Schema(type=genai.protos.Type.OBJECT, properties={})
            )
            
            tools = genai.protos.Tool(function_declarations=[
                ingest_raw_declaration,
                analyze_declaration,
                identify_declaration,
                prioritize_declaration,
                validate_req_declaration,
                generate_report_declaration,
                store_context_declaration,
                search_context_declaration,
                help_declaration,
                clear_declaration
            ])
            
            system_instruction = f"""B·∫°n l√† Business Analysis Assistant - Chuy√™n gia ph√¢n t√≠ch nghi·ªáp v·ª• v√† Use Case.

üéØ SCOPE NGHI√äM NG·∫∂T - CH·ªà h·ªó tr·ª£ Business Analysis & Use Case Analysis:
‚úÖ Business Requirements Analysis
‚úÖ Use Case Modeling & Specifications  
‚úÖ Context Diagram & Use Case Diagram
‚úÖ Stakeholder Analysis
‚úÖ Business Process Analysis
‚úÖ Requirements Prioritization

‚ùå KH√îNG h·ªó tr·ª£: Coding, Database Design, Technical Implementation, Testing, Project Management, General Chat

üìä Context hi·ªán t·∫°i:
- Conversation ID: {self.conversation_id}
- Messages loaded: {len(recent_messages)}
- Has summary: {bool(summary)}

üéØ Workflow t·ª± ƒë·ªông khi nh·∫≠n Business Requirements/Use Cases:
1. ingest_raw_requirements ‚Üí Thu th·∫≠p v√† chu·∫©n h√≥a requirements
2. analyze_stories ‚Üí Ph√¢n t√≠ch use cases, t√¨m actors, scenarios, issues
3. identify_requirements ‚Üí X√°c ƒë·ªãnh core business requirements & use cases
4. prioritize_requirements ‚Üí ∆Øu ti√™n theo business value (MoSCoW)
5. validate_requirements ‚Üí Validate completeness v√† consistency
6. generate_context_diagram ‚Üí T·∫°o Context Diagram + Use Case Diagram (Mermaid)
7. store_conversation_context ‚Üí L∆∞u analysis v√†o DB v·ªõi embeddings

‚ö†Ô∏è N·∫æU user h·ªèi NGO√ÄI SCOPE:
- L·ªãch s·ª± t·ª´ ch·ªëi: "Xin l·ªói, t√¥i ch·ªâ chuy√™n v·ªÅ Business Analysis v√† Use Case Analysis. T√¥i kh√¥ng th·ªÉ h·ªó tr·ª£ [topic]. B·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi v·ªÅ ph√¢n t√≠ch nghi·ªáp v·ª• ho·∫∑c use case kh√¥ng?"
- KH√îNG c·ªë g·∫Øng tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ coding, technical, ho·∫∑c topics kh√°c

üí° Phong c√°ch giao ti·∫øp:
- Th√¢n thi·ªán, nhi·ªát t√¨nh nh∆∞ m·ªôt Business Analyst chuy√™n nghi·ªáp
- Tr·∫£ l·ªùi t·ª± nhi√™n, sinh ƒë·ªông, kh√¥ng c·ª©ng nh·∫Øc
- Ch·ªß ƒë·ªông ƒë·ªÅ xu·∫•t c·∫£i thi·ªán requirements n·∫øu ph√°t hi·ªán thi·∫øu s√≥t
- Gi·∫£i th√≠ch insights t·ª´ analysis m·ªôt c√°ch d·ªÖ hi·ªÉu
- Khen ng·ª£i khi requirements ƒë∆∞·ª£c vi·∫øt r√µ r√†ng

üìù Input formats ƒë∆∞·ª£c h·ªó tr·ª£:
- Business Requirements: "The business needs to [objective] in order to [benefit]"
- User Stories: "As a [actor], I want to [action] so that [benefit]"
- Use Cases: "Actor: [who], Goal: [what], Scenario: [steps]"
- Functional Requirements: "The system shall/must [capability]"
- Business Rules: "When [condition] then [action]"
- Business Process: M√¥ t·∫£ quy tr√¨nh nghi·ªáp v·ª• hi·ªán t·∫°i ho·∫∑c mong mu·ªën
- Stakeholder Needs: Nhu c·∫ßu c·ªßa c√°c b√™n li√™n quan

üîß MCP Tools Available (ch·ªâ d√πng cho Business Analysis):
- ingest_raw_requirements: Thu th·∫≠p business requirements v√† use cases
- analyze_stories: Ph√¢n t√≠ch use cases, identify actors, scenarios, gaps
- identify_requirements: Extract core business requirements v√† use cases
- prioritize_requirements: ∆Øu ti√™n theo business value (MoSCoW method)
- validate_requirements: Validate completeness, consistency, testability
- generate_context_diagram: T·∫°o Context Diagram + Use Case Diagram (Mermaid)
- store_conversation_context: L∆∞u business analysis v√†o DB v·ªõi embeddings
- search_previous_context: T√¨m previous business analysis
- show_help, clear_requirements: Utilities

‚ö° H√†nh ƒë·ªông th√¥ng minh:
- T·ª± ƒë·ªông g·ªçi ingest_raw_requirements khi nh·∫≠n raw text t·ª´ user
- Chain c√°c MCP tools ƒë·ªÉ t·∫°o complete analysis pipeline
- T·ª± ƒë·ªông store summary + embeddings v√†o DB conversation sau khi ho√†n th√†nh
- Search trong user's conversations b·∫±ng semantic similarity (embeddings)
- Load existing context khi reconnect to conversation
- Present k·∫øt qu·∫£ v·ªõi insights v√† recommendations"""
            
            model = genai.GenerativeModel(
                MODEL,
                system_instruction=system_instruction,
                tools=[tools],
                generation_config={
                    "temperature": 0.8,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": 2048,
                }
            )
            
            # Build conversation history from loaded context (from DB)
            # Include summary as first context if available
            history = []
            
            if summary:
                # Add summary as system context
                history.append({
                    "role": "user",
                    "parts": [f"[Previous conversation summary]\n{summary[:1000]}..."]
                })
                history.append({
                    "role": "model",
                    "parts": ["I have the context from our previous conversation."]
                })
            
            # Add recent messages for immediate context
            for msg in recent_messages[-8:]:  # Last 8 messages
                role = "user" if msg["role"] == "user" else "model"
                history.append({"role": role, "parts": [msg["content"]]})
            
            chat = model.start_chat(history=history)
            response = await asyncio.to_thread(chat.send_message, message)
            
            # Handle function calls (may chain multiple tools)
            max_iterations = 10  # Prevent infinite loops
            iteration = 0
            
            while iteration < max_iterations:
                iteration += 1
                
                if not (response.candidates and response.candidates[0].content.parts):
                    break
                
                function_calls = []
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'function_call') and part.function_call:
                        function_calls.append(part.function_call)
                
                if not function_calls:
                    break
                
                # Execute all function calls
                function_responses = []
                for fc in function_calls:
                    tool_result = await self._execute_tool(fc.name, dict(fc.args))
                    function_responses.append(
                        genai.protos.Part(
                            function_response=genai.protos.FunctionResponse(
                                name=fc.name,
                                response=tool_result
                            )
                        )
                    )
                
                # Send all results back to Gemini
                response = await asyncio.to_thread(
                    chat.send_message,
                    genai.protos.Content(parts=function_responses)
                )
                
                # If Gemini has final text response, break
                if hasattr(response, 'text') and response.text:
                    break
            
            # Return response text
            if hasattr(response, 'text'):
                return response.text
            return str(response)
            
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
    
    async def _execute_tool(self, tool_name: str, args: dict) -> dict:
        """Execute MCP tool based on Gemini's function call."""
        try:
            from api.services import mcp_adapter
            
            if tool_name == "ingest_raw_requirements":
                items = args.get("items", [])
                if not items:
                    return {"error": "No items provided"}
                
                # Call MCP Collector: ingest_raw
                result = await asyncio.to_thread(
                    mcp_adapter.call_mcp,
                    "mcp_collector",
                    "ingest_raw",
                    {"items": items}
                )
                
                if result.get("error"):
                    return {"error": result.get("error")}
                
                chunks = result.get("response", {}).get("chunks", [])
                
                # Call MCP Collector: extract_stories
                stories_result = await asyncio.to_thread(
                    mcp_adapter.call_mcp,
                    "mcp_collector",
                    "extract_stories",
                    {"chunks": chunks}
                )
                
                if stories_result.get("error"):
                    return {"error": stories_result.get("error")}
                
                stories = stories_result.get("response", {}).get("stories", [])
                
                # Cache for summary
                self.last_pipeline_result["stories"] = stories
                
                return {
                    "success": True,
                    "message": f"‚úÖ Thu th·∫≠p {len(items)} requirements ‚Üí {len(stories)} stories",
                    "stories": stories,
                    "chunks": chunks
                }
            
            elif tool_name == "analyze_stories":
                stories = args.get("stories", [])
                if not stories:
                    return {"error": "No stories to analyze"}
                
                # Call MCP Analyzer
                result = await asyncio.to_thread(
                    mcp_adapter.call_mcp,
                    "mcp_analyzer",
                    "analyze_stories",
                    {"stories": stories, "options": {"use_llm": True}}
                )
                
                if result.get("error"):
                    return {"error": result.get("error")}
                
                analysis = result.get("response", {}).get("analysis", {})
                enriched_stories = result.get("response", {}).get("stories", stories)
                
                # Cache for summary
                self.last_pipeline_result["analysis"] = analysis
                self.last_pipeline_result["stories"] = enriched_stories
                
                return {
                    "success": True,
                    "message": f"üìä Ph√¢n t√≠ch: {analysis.get('summary', {}).get('total_issues', 0)} issues found",
                    "stories": enriched_stories,
                    "analysis": analysis
                }
            
            elif tool_name == "identify_requirements":
                stories = args.get("stories", [])
                if not stories:
                    return {"error": "No stories provided"}
                
                # Call MCP Requirement: identify
                result = await asyncio.to_thread(
                    mcp_adapter.call_mcp,
                    "mcp_requirement",
                    "identify_requirements",
                    {"stories": stories, "options": {"use_llm": True}}
                )
                
                if result.get("error"):
                    return {"error": result.get("error")}
                
                requirements = result.get("response", {}).get("requirements", [])
                
                # Cache for summary
                self.last_pipeline_result["requirements"] = requirements
                
                return {
                    "success": True,
                    "message": f"üéØ X√°c ƒë·ªãnh {len(requirements)} core requirements",
                    "requirements": requirements
                }
            
            elif tool_name == "prioritize_requirements":
                requirements = args.get("requirements", [])
                if not requirements:
                    return {"error": "No requirements to prioritize"}
                
                # Call MCP Requirement: prioritize
                result = await asyncio.to_thread(
                    mcp_adapter.call_mcp,
                    "mcp_requirement",
                    "prioritize",
                    {"requirements": requirements}
                )
                
                if result.get("error"):
                    return {"error": result.get("error")}
                
                prioritized = result.get("response", {}).get("requirements", [])
                
                return {
                    "success": True,
                    "message": f"‚≠ê ƒê√£ ∆∞u ti√™n {len(prioritized)} requirements",
                    "requirements": prioritized
                }
            
            elif tool_name == "validate_requirements":
                requirements = args.get("requirements", [])
                if not requirements:
                    return {"error": "No requirements to validate"}
                
                # Call MCP Validator
                result = await asyncio.to_thread(
                    mcp_adapter.call_mcp,
                    "mcp_validator",
                    "validate_requirements",
                    {"requirements": requirements}
                )
                
                if result.get("error"):
                    return {"error": result.get("error")}
                
                issues = result.get("response", {}).get("issues", [])
                
                # Cache for summary
                self.last_pipeline_result["validation_issues"] = issues
                
                return {
                    "success": True,
                    "message": f"‚úì Validation: {len(issues)} issues found" if issues else "‚úì All requirements valid",
                    "issues": issues,
                    "requirements": requirements  # Pass through for next step
                }
            
            elif tool_name == "generate_context_diagram":
                requirements = args.get("requirements", [])
                if not requirements:
                    return {"error": "No requirements for diagram"}
                
                # Call MCP Reporter
                result = await asyncio.to_thread(
                    mcp_adapter.call_mcp,
                    "mcp_reporter",
                    "generate_report",
                    {"requirements": requirements}
                )
                
                if result.get("error"):
                    return {"error": result.get("error")}
                
                report = result.get("response", {}).get("report", {})
                diagram = report.get("context_diagram", "")
                
                # Cache for summary
                self.last_pipeline_result["diagram"] = diagram
                self.last_pipeline_result["report"] = report
                
                return {
                    "success": True,
                    "message": "üé® Context diagram created",
                    "diagram": diagram,
                    "report": report,
                    "requirements": requirements  # Pass through for storage
                }
            
            elif tool_name == "store_conversation_context":
                summary = args.get("summary", "")
                requirements = args.get("requirements", self.last_pipeline_result.get("requirements", []))
                diagram = args.get("diagram", self.last_pipeline_result.get("diagram", ""))
                
                # Auto-generate summary if not provided
                if not summary:
                    summary = f"""Requirements Analysis Session
                    
Identified {len(requirements)} core requirements

Key Requirements:
{chr(10).join([f"- {r.get('title', 'Untitled')}" for r in requirements[:5]])}
"""
                
                # Prepare full context document for embedding
                context_text = f"""Summary: {summary}

Requirements Count: {len(requirements)}

Requirements:
{json.dumps(requirements, indent=2, ensure_ascii=False)}

Context Diagram:
{diagram}
"""
                
                # Generate embedding for semantic search
                embedding = await self._generate_embedding(context_text)
                
                # Save to conversation DB (primary storage)
                await self._save_conversation_summary(context_text, embedding)
                
                # Also store in vector MCP for additional search capabilities
                try:
                    context_id = f"conv_{self.conversation_id}_{int(datetime.utcnow().timestamp())}"
                    await asyncio.to_thread(
                        mcp_adapter.call_mcp,
                        "mcp_vector",
                        "ingest",
                        {
                            "ids": [context_id],
                            "texts": [context_text],
                            "metadatas": [{
                                "conversation_id": self.conversation_id,
                                "user_id": self.user_id,
                                "timestamp": datetime.utcnow().isoformat(),
                                "requirements_count": len(requirements),
                                "type": "requirements_analysis"
                            }]
                        }
                    )
                except Exception as e:
                    print(f"Warning: Vector MCP storage failed: {e}")
                
                return {
                    "success": True,
                    "message": "üíæ Context saved to DB with embeddings",
                    "summary_length": len(context_text),
                    "embedding_dim": len(embedding) if embedding else 0
                }
            
            elif tool_name == "search_previous_context":
                query = args.get("query", "")
                top_k = args.get("top_k", 5)
                
                if not query:
                    return {"error": "No query provided"}
                
                # Generate query embedding
                query_embedding = await self._generate_embedding(query)
                
                # Search in conversation DB using embeddings
                formatted_results = []
                
                try:
                    async with async_session() as db:
                        from sqlalchemy import select, func
                        from api.core.models import Conversation
                        
                        # Get all conversations with embeddings for this user
                        stmt = select(Conversation).where(
                            Conversation.user_id == self.user_id,
                            Conversation.summary_embedding.isnot(None),
                            Conversation.status == 1
                        )
                        result = await db.execute(stmt)
                        conversations = result.scalars().all()
                        
                        # Calculate cosine similarity with query
                        similarities = []
                        for conv in conversations:
                            if conv.summary_embedding and query_embedding:
                                # Cosine similarity
                                import numpy as np
                                conv_emb = np.array(conv.summary_embedding)
                                query_emb = np.array(query_embedding)
                                
                                # Normalize
                                conv_norm = conv_emb / np.linalg.norm(conv_emb)
                                query_norm = query_emb / np.linalg.norm(query_emb)
                                
                                similarity = np.dot(conv_norm, query_norm)
                                similarities.append((conv, similarity))
                        
                        # Sort by similarity and take top_k
                        similarities.sort(key=lambda x: x[1], reverse=True)
                        top_results = similarities[:top_k]
                        
                        # Format results
                        for i, (conv, sim) in enumerate(top_results, 1):
                            summary_preview = conv.summary[:500] + "..." if conv.summary and len(conv.summary) > 500 else conv.summary
                            formatted_results.append({
                                "rank": i,
                                "conversation_id": conv.id,
                                "conversation_name": conv.name,
                                "content": summary_preview,
                                "created_at": conv.created_at.isoformat() if conv.created_at else None,
                                "similarity": float(sim)
                            })
                
                except Exception as e:
                    # Fallback to vector MCP search
                    print(f"DB search failed, using vector MCP: {e}")
                    try:
                        result = await asyncio.to_thread(
                            mcp_adapter.call_mcp,
                            "mcp_vector",
                            "search",
                            {"query": query, "top_k": top_k}
                        )
                        
                        if not result.get("error"):
                            search_result = result.get("response", {}).get("result", {})
                            documents = search_result.get("documents", [[]])[0]
                            metadatas = search_result.get("metadatas", [[]])[0]
                            distances = search_result.get("distances", [[]])[0]
                            
                            for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances)):
                                formatted_results.append({
                                    "rank": i + 1,
                                    "content": doc[:500] + "..." if len(doc) > 500 else doc,
                                    "metadata": meta,
                                    "similarity": 1 - dist
                                })
                    except Exception as e2:
                        print(f"Vector MCP search also failed: {e2}")
                
                return {
                    "success": True,
                    "message": f"üîç Found {len(formatted_results)} previous contexts from DB",
                    "results": formatted_results
                }
            
            elif tool_name == "show_help":
                return {"success": True, "message": self._get_help()}
            
            elif tool_name == "clear_requirements":
                # Legacy function - no longer used (requirements in DB now)
                return {"success": True, "message": "‚úÖ ƒê√£ x√≥a t·∫•t c·∫£ requirements (legacy function)"}
            
            return {"error": f"Unknown tool: {tool_name}"}
            
        except Exception as e:
            return {"error": f"Tool execution failed: {str(e)}", "trace": traceback.format_exc()}

    def _get_help(self) -> str:
        """Return help text."""
        return """üéØ Business Analysis & Use Case Assistant

üìã SCOPE - Ch·ªâ h·ªó tr·ª£:
‚úÖ Business Requirements Analysis
‚úÖ Use Case Analysis & Modeling
‚úÖ Context Diagram & Use Case Diagram
‚úÖ Stakeholder Analysis
‚úÖ Business Process Analysis

‚ùå KH√îNG h·ªó tr·ª£: Coding, Database, Technical Implementation, Testing

üìù Input Formats:
‚Ä¢ Business Requirement: "The business needs to [objective]"
‚Ä¢ User Story: "As a [actor], I want to [action] so that [benefit]"
‚Ä¢ Use Case: "Actor: [who], Goal: [what], Main Flow: [steps]"
‚Ä¢ Business Process: M√¥ t·∫£ quy tr√¨nh nghi·ªáp v·ª•
‚Ä¢ Stakeholder Need: Nhu c·∫ßu c·ªßa c√°c b√™n li√™n quan

üîÑ Workflow:
1. Thu th·∫≠p requirements/use cases
2. Ph√¢n t√≠ch nghi·ªáp v·ª•, identify actors, scenarios
3. T√¨m gaps, conflicts, ambiguity
4. ∆Øu ti√™n theo business value (MoSCoW)
5. T·∫°o Context Diagram + Use Case Diagram
6. Generate Use Case Specifications

üí° Example:
"Ph√¢n t√≠ch use case ƒëƒÉng nh·∫≠p cho h·ªá th·ªëng"
"T√¥i c·∫ßn context diagram cho ·ª©ng d·ª•ng qu·∫£n l√Ω b√°n h√†ng"
"Ph√¢n t√≠ch business requirements cho t√≠nh nƒÉng thanh to√°n"

ÔøΩ Search Previous:
"T√¨m analysis v·ªÅ authentication use case"
"Show me previous payment requirements"

üíæ T·∫•t c·∫£ analysis ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông v·ªõi embeddings ƒë·ªÉ recall sau."""
